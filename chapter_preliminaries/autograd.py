import torch

x = torch.arange(4.0)
#print(x)
#ä¸ä¼šæ¯æ¬¡æ±‚å¯¼éƒ½åˆ†é…æ–°çš„å†…å­˜
x.requires_grad_(True) #ç­‰ä»·çš„å†™æ³•ï¼šx = torch.arange(4.0, requires_grad=True)

##########grad#############
#å½“ requires_grad=True æ—¶ï¼ŒPyTorch ä¼šè®°å½•æ‰€æœ‰åœ¨è¯¥å¼ é‡ä¸Šçš„æ“ä½œ
#è¿™äº›æ“ä½œä¼šæ„å»ºä¸€ä¸ªè®¡ç®—å›¾ï¼Œç”¨äºåç»­çš„åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
#.grad å±æ€§ä¼šå­˜å‚¨åå‘ä¼ æ’­æ—¶è®¡ç®—çš„æ¢¯åº¦å€¼
#æ³¨æ„ï¼šåªæœ‰æµ®ç‚¹ç±»å‹å¼ é‡æ”¯æŒæ±‚å¯¼æ“ä½œï¼ï¼ï¼
#é»˜è®¤æƒ…å†µä¸‹ï¼Œrequires_grad=False
#å¶å­èŠ‚ç‚¹ï¼ˆå¦‚è¾“å…¥å¼ é‡ï¼‰çš„ .grad å±æ€§ä¼šåœ¨åå‘ä¼ æ’­åè¢«å¡«å……
#ä¸­é—´ç»“æœçš„æ¢¯åº¦ä¼šè¢«è‡ªåŠ¨é‡Šæ”¾ä»¥èŠ‚çœå†…å­˜
###########################

print(x.grad) #æ­¤æ—¶è¾“å‡ºä¸ºNone
#Let y = 2 \mathbf{x}^\top \mathbf{x}
y = 2 * torch.dot(x, x) #å‘é‡xçš„è½¬ç½®ä¹˜xï¼Œç›¸å½“äºxä¸è‡ªèº«ç‚¹ç§¯, æœ€åå¾—åˆ°ä¸€ä¸ªæ ‡é‡
print(y) #tensor(28., grad_fn=<MulBackward0>)
#è°ƒç”¨åå‘ä¼ æ’­å‡½æ•°è‡ªåŠ¨è®¡ç®—yå…³äºxæ¯ä¸ªåˆ†é‡çš„æ¢¯åº¦
y.backward() #backpropagate
print(x.grad) #tensor([ 0.,  4.,  8., 12.])
print(x.grad == 4 * x)

######backward()##############
#è®¡ç®—å›¾æ„å»º:
#å½“æ‰§è¡Œå‰å‘è®¡ç®—æ—¶ï¼ˆå¦‚ y = 2 * torch.dot(x, x)ï¼‰ï¼ŒPyTorch è‡ªåŠ¨æ„å»ºè®¡ç®—å›¾
#æ¯ä¸ªæ“ä½œéƒ½è¢«è®°å½•ä¸ºå›¾ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹
#è®¡ç®—å›¾è®°å½•äº†ä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´è®¡ç®—è·¯å¾„
#æ¢¯åº¦è®¡ç®—:
#è°ƒç”¨ backward() æ—¶ï¼ŒPyTorch ä»è¾“å‡ºèŠ‚ç‚¹ï¼ˆè¿™é‡Œæ˜¯ yï¼‰å¼€å§‹
#ä½¿ç”¨é“¾å¼æ³•åˆ™ï¼Œé€å±‚è®¡ç®—æ¯ä¸ªå˜é‡çš„æ¢¯åº¦
#æ¢¯åº¦æŒ‰ç…§è®¡ç®—å›¾çš„åå‘é¡ºåºä¼ æ’­
#ç»“æœå­˜å‚¨:
#å¯¹äº requires_grad=True çš„å¶å­èŠ‚ç‚¹ï¼ˆå¦‚ xï¼‰ï¼Œæ¢¯åº¦è¢«å­˜å‚¨åœ¨å…¶ .grad å±æ€§ä¸­
#ä¸­é—´èŠ‚ç‚¹çš„æ¢¯åº¦è®¡ç®—å®Œåä¼šè¢«é‡Šæ”¾ä»¥èŠ‚çœå†…å­˜
###########################

x.grad.zero_() #åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œpytorchä¼šç´¯è®¡æ¢¯åº¦ï¼Œéœ€è¦æ¸…é™¤ä¹‹å‰çš„å€¼
y = x.sum()
y.backward()
print(x.grad) #tensor([1, 1, 1, 1])
#éæ ‡é‡å˜é‡çš„backpropagate
x.grad.zero_()
y = x * x #Recall: * æŒ‰å…ƒç´ ä¹˜æ³•ï¼Œå¾—åˆ°ä¸€ä¸ªä¸xå½¢çŠ¶ç›¸åŒçš„å‘é‡
y.backward(torch.ones_like(x)) #æœ¬ä¾‹ä¸­æ±‚æ¯ä¸ªæ ·æœ¬çš„åå¯¼æ•°ä¹‹å’Œ
#ç†è§£ï¼šè¿™é‡Œçš„â€œåå¯¼æ•°ä¹‹å’Œâ€åº”æŒ‡å¯¹å‘é‡ğ‘¥æ¯ä¸ªåˆ†é‡ğ‘¥ğ‘–æ±‚åå¯¼æ•°çš„å’Œ
print(x.grad)
#####å¯¹å‘é‡/æ ‡é‡è°ƒç”¨backward()#######
# å¯¹éæ ‡é‡è°ƒç”¨backwardéœ€è¦ä¼ å…¥ä¸€ä¸ªgradientå‚æ•°ï¼Œè¯¥å‚æ•°æŒ‡å®šå¾®åˆ†å‡½æ•°å…³äºselfçš„æ¢¯åº¦ã€‚
# æœ¬ä¾‹åªæƒ³æ±‚åå¯¼æ•°çš„å’Œï¼Œæ‰€ä»¥ä¼ é€’ä¸€ä¸ª1çš„æ¢¯åº¦æ˜¯åˆé€‚çš„
# è¯¦è§hhh/d2l-zh/pytorch:Jupyter notebook
##############################

#æ³¨æ„ï¼šåŸæ¥æ±‚å¯¼ç»“æœåº”è¯¥æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œè¿™äº›ç°è±¡ä¼šåœ¨ä»¥åæ·±åº¦å­¦ä¹ ä¸­å‡ºç°
#[0, 0, 0, 0]
#[0, 2, 0, 0]
#[0, 0, 4, 0]
#[0, 0, 0, 6]

#åˆ†ç¦»è®¡ç®—: è€ƒè™‘y = y(x),z = z(x, y),éœ€è¦è®¡ç®—zå…³äºxçš„æ¢¯åº¦ï¼Œæœ‰æ—¶å¸Œæœ›å°†yè§†ä½œå¸¸æ•°ï¼Œå³åªè€ƒè™‘xåœ¨yè¢«è®¡ç®—åå‘æŒ¥çš„ä½œç”¨
x.grad.zero_()
y = x * x
u = y.detach() #åˆ†ç¦»yè¿”å›ä¸€ä¸ªæ–°çš„å˜é‡u.è¯¥å˜é‡ä¸yå…·æœ‰ç›¸åŒçš„å€¼,ä½†ä¸¢å¼ƒè®¡ç®—å›¾ä¸­å¦‚ä½•è®¡ç®—yçš„ä»»ä½•ä¿¡æ¯.å³æ¢¯åº¦ä¸ä¼šå‘åæµç»uåˆ°x
z = u * x
z.sum().backward()
print(x.grad) #è¾“å‡ºtensor([0, 1, 4, 9]) ,å³å‘é‡u
print(x.grad == u) #tensor([True, True, True, True])

x.grad.zero_()
y.sum().backward()
print(x.grad) #tensor([0., 2., 4., 6.])

#å¯¹æ¯”ï¼šä¸ä½¿ç”¨u, z = y * x = x * x * x
x.grad.zero_()
#z = y * x
#z.sum().backward()
#print(x.grad) #tensor([ 0.,  3., 12., 27.])å³3 x^{2}

#######æŠ¥é”™#############æŠ¥é”™åŸå› ï¼šä¸Šä¸‰è¡Œä¸­yå·²ç»è¢«åå‘ä¼ æ’­è¿‡äº†
#RuntimeError: Trying to backward through the graph a second time 
#(or directly access saved tensors after they have already been freed).
#Saved intermediate values of the graph are freed when you call 
#.backward() or autograd.grad(). 
#Specify retain_graph=True if you need to backward through the graph 
#a second time or if you need to access saved tensors
#after calling backward.
#######################

y = x * x #è§£å†³ï¼šé‡æ–°å†™ä¸€é
z = y * x
z.sum().backward()
print(x.grad)

#Pythonæ§åˆ¶æµ(æ¡ä»¶ï¼Œå¾ªç¯æˆ–ä»»æ„å‡½æ•°è°ƒç”¨)ä¸­çš„æ¢¯åº¦è®¡ç®—
def f(a):
    b = a * 2
    while b.norm() < 1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    
    return c

a = torch.randn(size=(2, 3), requires_grad=True) #randnæœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒ
#size=?æ§åˆ¶å½¢çŠ¶()ç©ºæ‹¬å·ï¼Œæ ‡é‡;(2,)é•¿åº¦ä¸º2;(2, 3)2 * 3çŸ©é˜µ;(2, 3, 4)ä¾æ­¤ç±»æ¨
d = f(a) #fåœ¨è¾“å…¥aä¸­æ˜¯åˆ†æ®µçº¿æ€§çš„ï¼Œå¯¹ä»»æ„aï¼Œå­˜åœ¨k,f(a) = k * a
d.backward(torch.ones(2, 3))
print(a.grad == d / a)

#######Exercises##########
def Euclidean(a):
    for i in range(2):
       a = a * a
    return a

matrix_2 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=torch.float32, requires_grad=True)
matrix_3 = torch.tensor([[1], [2], [3]], dtype=torch.float32, requires_grad=True)
d = Euclidean(matrix_2)
d.backward(torch.ones_like(matrix_2))
print(matrix_2.grad)
d = Euclidean(matrix_3)
d.backward(torch.ones_like(matrix_3))
print(matrix_3.grad) #4 x^{3}, xä¸ºåˆ†é‡

##########################